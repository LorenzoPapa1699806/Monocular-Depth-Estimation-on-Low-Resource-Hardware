{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMS15udrIpBN"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YITlls9wnK1-"
      },
      "source": [
        "def plot_depth_map(dm):\n",
        "    MIN_DEPTH = 0.0\n",
        "    MAX_DEPTH = min(1000, np.percentile(dm, 99))\n",
        "\n",
        "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
        "    cmap = plt.cm.plasma_r\n",
        "    cmap.set_bad(color='black')\n",
        "\n",
        "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
        "\n",
        "\n",
        "def resize_keeping_aspect_ratio(img, base, interpolation='area'):\n",
        "    '''\n",
        "    Resize the image to a defined length manteining its proportions\n",
        "    Scaling the shortest side of the image to a fixed 'base' length'\n",
        "    '''\n",
        "\n",
        "    if img.shape[0] <= img.shape[1]:\n",
        "        basewidth = int(base)\n",
        "        wpercent = (basewidth/float(img.shape[0]))\n",
        "        hsize = int((float(img.shape[1])*float(wpercent)))\n",
        "        img = tf.image.resize(img, [basewidth, hsize], method=interpolation, antialias=True)\n",
        "    else:\n",
        "        baseheight = int(base)\n",
        "        wpercent = (baseheight/float(img.shape[1]))\n",
        "        wsize = int((float(img.shape[0])*float(wpercent)))\n",
        "        img = tf.image.resize(img, [wsize, baseheight], method=interpolation, antialias=True)\n",
        "\n",
        "    return np.array(img)\n",
        "    \n",
        "\n",
        "def get_zipped_dataset(dataset_path_zipped):\n",
        "    from zipfile import ZipFile\n",
        "    with ZipFile(dataset_path_zipped, 'r') as zip: \n",
        "        print('Extracting all the files now...') \n",
        "        zip.extractall() \n",
        "        print('Done!\\n')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQgtVbMESZNt"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0xeToU0JrBe"
      },
      "source": [
        "class DataLoader():\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        self.dataset = path\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.info = 0\n",
        "    \n",
        "    def get_test_dataset(self):\n",
        "        img_path = self.dataset + 'eigen_test_rgb.npy'\n",
        "        depth_path = self.dataset + 'eigen_test_depth.npy'\n",
        "        crop = self.dataset + 'eigen_test_crop.npy'\n",
        "\n",
        "        rgb = np.load(img_path)\n",
        "        depth = np.load(depth_path)\n",
        "        crop = np.load(crop)\n",
        "\n",
        "        self.x = rgb\n",
        "        self.y = depth\n",
        "\n",
        "        self.info = rgb.shape[0]\n",
        "\n",
        "        return self.info\n",
        "\n",
        "\n",
        "    def load_image(self, W_IMG_reduced_size=192, W_D_reduced_size=48, index=None):    \n",
        "        if index is None:\n",
        "            index = np.random.randint(0, self.info)\n",
        "\n",
        "        # Load RGB Image\n",
        "        img = self.x[index]\n",
        "\n",
        "        # Load Depth Image\n",
        "        depth = np.expand_dims(self.y[index], axis=-1)\n",
        "            \n",
        "        # Resize the image to a square image manteining the proportion\n",
        "        img = resize_keeping_aspect_ratio(img, W_IMG_reduced_size)\n",
        "        depth = resize_keeping_aspect_ratio(depth, W_D_reduced_size)\n",
        "        \n",
        "        return img/255, depth*100, index"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsT7EMWPQoi5"
      },
      "source": [
        "# Batch maker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc4Cduh9Qoi9"
      },
      "source": [
        "def get_batch_evaluation(batch_size=1, index=None):\n",
        "\n",
        "    input = np.zeros((batch_size, 192, 256, 3))\n",
        "    target = np.zeros((batch_size, 48, 64, 1))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img, depth, _ = dataset_test.load_image(index=index)\n",
        "        input[i, :, :, :] = np.expand_dims(img, axis=0)\n",
        "        target[i, :, :, :] = np.expand_dims(depth, axis=0)\n",
        "    \n",
        "    return input, target"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaIHy6tGNM_v"
      },
      "source": [
        "# Start the Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_stfOuv_d-9F"
      },
      "source": [
        "import os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "DATASET_TEST = []\n",
        "SIZE_TEST = 0\n",
        "path_weigths = root + '../Models/'\n",
        "path_dataset_test = root + '../Dataset/nyu_test.zip'\n",
        "\n",
        "# TFL\n",
        "!mkdir 'TF_tflite_models/'\n",
        "tf_ligth_root = '../TF_tflite_models/'\n",
        "TFLITE_MODEL = tf_ligth_root + 'tf_lite_model.tflite'\n",
        "    \n",
        "# Unzip and Load the Dataset\n",
        "zipped_test = get_zipped_dataset(path_dataset_test)\n",
        "dataset_test = DataLoader('../')\n",
        "test_samples = dataset_test.get_test_dataset()\n",
        "DATASET_TEST.append(dataset_test)\n",
        "SIZE_TEST += test_samples\n",
        "print('There are {} images for the Test'.format(test_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvIMJXvDk0RY"
      },
      "source": [
        "# Load the Model\n",
        "model = tf.keras.models.load_model(path_weigths + 'model.h5', compile=False)\n",
        "print('Model loaded\\n')\n",
        "\n",
        "# Get the concrete function from the Keras model.\n",
        "run_model = tf.function(lambda x : model(x))\n",
        "\n",
        "# Save the concrete function.\n",
        "concrete_func = run_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converted_tflite_model = converter.convert()\n",
        "open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\n",
        "\n",
        "print(\"TFLite models and their sizes:\")\n",
        "!ls \"TF_tflite_models\" -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfspBz1_la88"
      },
      "source": [
        "# Load TFLite model and see some details about input/output\n",
        "tflite_interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
        "\n",
        "# Learn about its input and output details\n",
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "#Let's resize input and output tensors, so we can make predictions for batch of images.\n",
        "tflite_interpreter.resize_tensor_input(input_details[0]['index'], (1, 192, 256, 3))\n",
        "tflite_interpreter.resize_tensor_input(output_details[0]['index'], (1, 48, 64, 1))\n",
        "tflite_interpreter.allocate_tensors()\n",
        "\n",
        "# Set input and output details\n",
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", input_details[0]['name'])\n",
        "print(\"shape:\", input_details[0]['shape'])\n",
        "print(\"type:\", input_details[0]['dtype'])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", output_details[0]['name'])\n",
        "print(\"shape:\", output_details[0]['shape'])\n",
        "print(\"type:\", output_details[0]['dtype'])\n",
        "\n",
        "# Compute predictions\n",
        "in_img, gt_depth = get_batch_evaluation(1, index=532)\n",
        "\n",
        "tflite_interpreter.set_tensor(input_details[0]['index'], np.array(in_img, dtype=np.float32))\n",
        "\n",
        "# Infer\n",
        "t1 = time.time()\n",
        "tflite_interpreter.invoke()\n",
        "passed_time = time.time() - t1\n",
        "print('Elapsed Time = {}\\nTime for image: {}'.format(passed_time, (passed_time)))\n",
        "\n",
        "# Prediction\n",
        "tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
        "img_pred, cmap_pred, _, _ = plot_depth_map(tflite_model_predictions)\n",
        "plt.imshow(tf.squeeze(img_pred), cmap=cmap_pred, vmin=0.0)\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_xlabel('cm', size=10, rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}